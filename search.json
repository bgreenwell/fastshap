[{"path":"/articles/fastshap.html","id":"local-explanations","dir":"Articles","previous_headings":"","what":"Local explanations","title":"fastshap","text":"illustrate simplest use Shapley values quantifying feature contributions, need observation predict. can use observation training set, ’ll construct observation new passenger. Everyone, meet Jack: Note fastshap, like many machine learning interpretability packages (e.g., iml), requires user-specified prediction wrapper; , simple function tells fastshap extract appropriate predictions fitted model. case, want explain Jack’s likelihood survival, prediction wrapper1 needs return conditional probability surviving fitted ranger object; see ?ranger::predict.ranger details: Yikes, Jack isn’t predicted fared well voyage, least compared baseline (.e., average training prediction)! Can try understand Jack’s predicted likelihood survival much smaller average? course, difference Shapley-based feature contributions help explain. illustrate, ’ll use explain() function estimate jack features2 (.e., age sex) contributed difference: fastshap package uses efficient version Monte-Carlo (MC) algorithm described @strumbelj-2014-explaining. Consequently, stability accuracy, feature contributions computed many times results averaged together. accomplish , simply set nsim argument reasonably high value (.e., much can computationally afford). compute 1000 Shapley-based feature contributions Jack average results: Note MC approach used fastshap (packages) produce Shapley-based feature contributions satisfy efficiency property; , won’t add difference corresponding prediction baseline (.e., average training prediction). However, borrowing trick popular Python shap library, can use regression-based adjustment correct sum. , simply set adjust = TRUE call explain()3: Next, can use shapviz package produce several useful visualizations either vector matrix Shapley values. , create simple waterfall chart visualize Jack’s features contributed relatively low predicted probability surviving:  Clearly, fact Jack male, third-class passenger contributed pushing predicted probability survival baseline. Force plots another popular way visualize Shapley values explaining single prediction:  Although force plots cool, waterfall charts seem much effective way visualizing feature contributions single prediction; especially ’s large number features.","code":"jack.dawson <- data.frame(   #survived = 0L,  # in case you haven't seen the movie   pclass = 3L,     # third-class passenger   age = 20.0,      # twenty years old   sex = factor(\"male\", levels = c(\"female\", \"male\")),  # male   sibsp = 0L,      # no siblings/spouses aboard   parch = 0L       # no parents/children aboard ) pfun <- function(object, newdata) {  # prediction wrapper   unname(predict(object, data = newdata)$predictions[, \"yes\"]) }  # Compute Jack's predicted likelihood of survival (jack.prob <- pfun(rfo, newdata = jack.dawson)) ## [1] 0.1330587 # Average prediction across all passengers (baseline <- mean(pfun(rfo, newdata = t1))) ## [1] 0.3815068 # Difference between Jack and average (difference <- jack.prob - baseline) ## [1] -0.2484481 X <- subset(t1, select = -survived)  # features only set.seed(2113)  # for reproducibility (ex.jack <- explain(rfo, X = X, pred_wrapper = pfun, newdata = jack.dawson)) ##      pclass          age sex      sibsp parch ## [1,]      0 -0.006721834   0 0.03017177     0 set.seed(2129)  # for reproducibility (ex.jack <- explain(rfo, X = X, pred_wrapper = pfun, newdata = jack.dawson,                     nsim = 1000)) ##           pclass          age        sex       sibsp       parch ## [1,] -0.07878601 -0.009507426 -0.1417691 0.005069262 -0.01201627 set.seed(2133)  # for reproducibility (ex.jack.adj <- explain(rfo, X = X, pred_wrapper = pfun, newdata = jack.dawson,                         nsim = 1000, adjust = TRUE)) ##           pclass         age        sex       sibsp       parch ## [1,] -0.07299993 -0.02063907 -0.1491682 0.007971709 -0.01361257 # Sanity check sum(ex.jack.adj)  # should be -0.2484481 ## [1] -0.2484481 library(shapviz)  shv <- shapviz(ex.jack.adj, X = jack.dawson, baseline = baseline) sv_waterfall(shv) sv_force(shv)"},{"path":"/articles/fastshap.html","id":"global-explanations","dir":"Articles","previous_headings":"","what":"Global explanations","title":"fastshap","text":"Aside explaining individual prediction (.e., local explanation), can useful aggregate results several (.e., training predictions) overall global summary model (.e., global explanations). However, computing Shapley values large number observations can quite computationally expensive, especially using MC approach. However, fastshap quite efficient compared alternative implementations4. code chunk computes Shapley explanations passenger training data using 1000 MC repetitions, coerces resulting matrix tibble (nicer printing). Note set optional argument shap_only = FALSE . convenience argument working shapviz; short, setting FALSE return list containing Shapely values, feature values, baseline (can used shapviz’s plotting functions). common global measure computed Shapley values Shapley-based feature importance scores, nothing mean absolute value features contribution column:  Another common global visualization Shapley dependence plot, akin partial dependence plot. , ’ll look dependence feature contribution age input value:","code":"set.seed(2224)  # for reproducibility ex.t1 <- explain(rfo, X = X, pred_wrapper = pfun, nsim = 100, adjust = TRUE,                  shap_only = FALSE) tibble::as_tibble(ex.t1$shapley_values) ## # A tibble: 1,309 × 5 ##    pclass      age     sex      sibsp    parch ##     <dbl>    <dbl>   <dbl>      <dbl>    <dbl> ##  1  0.231  0.00815  0.315   0.0205    -0.00924 ##  2  0.140  0.330   -0.0767  0.00589    0.0772  ##  3  0.161  0.0293   0.126  -0.0263    -0.0252  ##  4  0.214 -0.0231  -0.186   0.0156     0.00436 ##  5  0.193 -0.0328   0.286  -0.0183    -0.0468  ##  6  0.171 -0.0320  -0.197   0.0000595 -0.00303 ##  7  0.176 -0.127    0.345  -0.0103     0.00340 ##  8  0.151 -0.0713  -0.189  -0.00363   -0.0138  ##  9  0.239  0.00252  0.296   0.0437     0.00567 ## 10  0.112 -0.111   -0.210   0.00154   -0.00452 ## # ℹ 1,299 more rows shv.global <- shapviz(ex.t1) sv_importance(shv) sv_dependence(shv.global, v = \"age\")"},{"path":"/articles/fastshap.html","id":"parallel-processing","dir":"Articles","previous_headings":"","what":"Parallel processing","title":"fastshap","text":"explain() function computes Shapley values one column time (efficient way). However, lot features, may beneficial run explain() parallel across columns. Since explain() uses foreach loop features computing Shapley values, can use parallel backend support; details, see parallel execution section “Using foreach package” vignette, can view vignette(\"foreach\", package = \"foreach\"). illustrate, ’ll compute Shapley values random forest fit Ames housing data available AmesHousing package: , ’ll define required prediction wrapper call explain() function without passing anything newdata (.e., Shapley values predictions every row X computed): Honestly, bad 50 MC repetitions data set 80 features 2930 rows! comparison, ’ll run computation, time parallel using doParallel package execute across 12 cores: bad speedup! Since didn’t set shap_only=FALSE call explain(), ’ll need pass corresponding feature values baseline interfacing shapviz. default, long adjust = TRUE, baseline automatically computed average training prediction (whatever suitable background feature set provided via X) stored \"baseline\" returned matrix shap_only=TRUE, \"baseline\" component returned object shap_only=FALSE. instance, construct Shapley-based variable importance plot ex.ames.par object, can simply following:  Similar Shapley-based dependence plots:","code":"ames <- as.data.frame(AmesHousing::make_ames()) X <- subset(ames, select = -Sale_Price)  # features only  # Fit a random forest set.seed(102) (rfo <- ranger(Sale_Price ~ ., data =  ames, write.forest = TRUE)) ## Ranger result ##  ## Call: ##  ranger(Sale_Price ~ ., data = ames, write.forest = TRUE)  ##  ## Type:                             Regression  ## Number of trees:                  500  ## Sample size:                      2930  ## Number of independent variables:  80  ## Mtry:                             8  ## Target node size:                 5  ## Variable importance mode:         none  ## Splitrule:                        variance  ## OOB prediction error (MSE):       622600969  ## R squared (OOB):                  0.9024424 # Prediction wrapper pfun <- function(object, newdata) {   predict(object, data = newdata)$predictions }  # Without parallelism set.seed(1706) system.time({  # estimate run time   ex.ames.nonpar <- explain(rfo, X = X, pred_wrapper = pfun, nsim = 50,                             adjust = TRUE) }) ##     user   system  elapsed  ## 2402.172  216.711  898.679 library(doParallel)  # With parallelism registerDoParallel(cores = 12)  # use forking with 12 cores set.seed(5038) system.time({  # estimate run time   ex.ames.par <- explain(rfo, X = X, pred_wrapper = pfun, nsim = 50,                           adjust = TRUE, parallel = TRUE) }) ##    user  system elapsed  ##   0.948   0.632 265.087 baseline <- attr(ex.ames.par, \"baseline\") shv <- shapviz(ex.ames.par, X = X, baseline = baseline) sv_importance(shv) sv_dependence(shv, v = \"Gr_Liv_Area\", alpha = 0.3)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Brandon Greenwell. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greenwell B (2023). fastshap: Fast Approximate Shapley Values. https://github.com/bgreenwell/fastshap, https://bgreenwell.github.io/fastshap/.","code":"@Manual{,   title = {fastshap: Fast Approximate Shapley Values},   author = {Brandon Greenwell},   year = {2023},   note = {https://github.com/bgreenwell/fastshap, https://bgreenwell.github.io/fastshap/}, }"},{"path":"/index.html","id":"fastshap-","dir":"","previous_headings":"","what":"Fast Approximate Shapley Values","title":"Fast Approximate Shapley Values","text":"goal fastshap provide efficient speedy approach (least relative implementations) computing approximate Shapley values, help explain predictions machine learning model.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fast Approximate Shapley Values","text":"","code":"# Install the latest stable version from CRAN: install.packages(\"fastshap\")  # Install the latest development version from GitHub: if (!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"bgreenwell/fastshap\")"},{"path":"/reference/bin.html","id":null,"dir":"Reference","previous_headings":"","what":"Bin a numeric vector — bin","title":"Bin a numeric vector — bin","text":"Function bin numeric vector","code":""},{"path":"/reference/bin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bin a numeric vector — bin","text":"","code":"bin(x, n_bins)"},{"path":"/reference/bin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bin a numeric vector — bin","text":"x numeric vector. Integer specifying number bins split x .","code":""},{"path":"/reference/explain.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast approximate Shapley values — explain","title":"Fast approximate Shapley values — explain","text":"Compute fast (approximate) Shapley values set features using Monte Carlo algorithm described Strumbelj Igor (2014). efficient algorithm tree-based models, commonly referred Tree SHAP, also supported lightgbm xgboost models; see Lundberg et. al. (2020) details.","code":""},{"path":"/reference/explain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast approximate Shapley values — explain","text":"","code":"explain(object, ...)  # S3 method for default explain(   object,   feature_names = NULL,   X = NULL,   nsim = 1,   pred_wrapper = NULL,   newdata = NULL,   adjust = FALSE,   baseline = NULL,   shap_only = TRUE,   parallel = FALSE,   ... )  # S3 method for lm explain(   object,   feature_names = NULL,   X,   nsim = 1,   pred_wrapper,   newdata = NULL,   adjust = FALSE,   exact = FALSE,   baseline = NULL,   shap_only = TRUE,   parallel = FALSE,   ... )  # S3 method for xgb.Booster explain(   object,   feature_names = NULL,   X = NULL,   nsim = 1,   pred_wrapper,   newdata = NULL,   adjust = FALSE,   exact = FALSE,   baseline = NULL,   shap_only = TRUE,   parallel = FALSE,   ... )  # S3 method for lgb.Booster explain(   object,   feature_names = NULL,   X = NULL,   nsim = 1,   pred_wrapper,   newdata = NULL,   adjust = FALSE,   exact = FALSE,   baseline = NULL,   shap_only = TRUE,   parallel = FALSE,   ... )"},{"path":"/reference/explain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast approximate Shapley values — explain","text":"object fitted model object (e.g., ranger::ranger(), xgboost::xgboost(), earth::earth() object, name ). ... Additional optional arguments passed foreach::foreach() whenever parallel = TRUE. example, may need supply additional packages parallel task depends via .packages argument foreach::foreach(). NOTE: foreach::foreach()'s .combine argument already set internally explain(), passing via ... argument likely result error. feature_names Character string giving names predictor variables (.e., features) interest. NULL (default) taken column names X. X matrix-like R object (e.g., data frame matrix) containing feature columns training data (suitable background data set). NOTE: argument required whenever exact = FALSE. nsim number Monte Carlo repetitions use estimating Shapley value (used exact = FALSE). Default 1. NOTE: obtain accurate results, nsim set large feasibly possible. pred_wrapper Prediction function requires two arguments, object newdata. NOTE: argument required whenever exact = FALSE. output function determined according : Regression numeric vector predicted outcomes. Binary classification vector predicted class probabilities reference class. Multiclass classification vector predicted class probabilities reference class. newdata matrix-like R object (e.g., data frame matrix) containing feature columns observation(s) interest; , observation(s) want compute explanations . Default NULL produce approximate Shapley values rows X (.e., training data). adjust Logical indicating whether adjust sum estimated Shapley values satisfy local accuracy property; , equal difference model's prediction sample average prediction training data (.e., X). Default FALSE setting TRUE requires nsim > 1. baseline Numeric baseline use adjusting computed Shapley values achieve local accuracy. Adjusted Shapley values single prediction (fx) sum difference fx - baseline. Defaults NULL, corresponds average predictions computed X, zero otherwise (.e., additional predictions computed baseline attribute output set zero). shap_only Logical indicating whether include additional output useful plotting (.e., newdata baseline value.). convenient, example, using shapviz::shapviz() plotting. Default TRUE. parallel Logical indicating whether compute approximate Shapley values parallel across features; default FALSE. NOTE: setting parallel = TRUE requires setting appropriate (.e., system-specific) parallel backend described foreach; details, see vignette(\"foreach\", package = \"foreach\") R. exact Logical indicating whether compute exact Shapley values. Currently available stats::lm(), xgboost::xgboost(), lightgbm::lightgbm() objects. Default FALSE. Note setting exact = TRUE return explanations stats::terms() stats::lm() object. Default FALSE.","code":""},{"path":"/reference/explain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast approximate Shapley values — explain","text":"shap_only = TRUE (default), matrix returned one column feature specified feature_names (feature_names = NULL, default, one column feature X) one row observation newdata (newdata = NULL, default, one row observation X). Additionally, returned matrix attribute called \"baseline\" containing baseline value. shap_only = FALSE, list returned three components: shapley_values - matrix Shapley values (described ); feature_values - corresponding feature values (plotting shapviz::shapviz()); baseline - corresponding baseline value (plotting shapviz::shapviz()).","code":""},{"path":"/reference/explain.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fast approximate Shapley values — explain","text":"Setting exact = TRUE linear model (.e., stats::lm() stats::glm() object) assumes input features independent. Also, setting adjust = TRUE experimental follow approach shap.","code":""},{"path":"/reference/explain.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast approximate Shapley values — explain","text":"Strumbelj, E., Igor K. (2014). Explaining prediction models individual predictions feature contributions. Knowledge information systems, 41(3), 647-665. Lundberg, S. M., Erion, G., Chen, H., DeGrave, ., Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., Lee, Su-(2020). local explanations global understanding explainable AI trees. Nature Machine Intelligence, 2(1), 2522–5839.","code":""},{"path":[]},{"path":"/reference/explain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast approximate Shapley values — explain","text":"","code":"# # A projection pursuit regression (PPR) example #  # Load the sample data; see ?datasets::mtcars for details data(mtcars)  # Fit a projection pursuit regression model fit <- lm(mpg ~ ., data = mtcars)  # Compute approximate Shapley values using 10 Monte Carlo simulations set.seed(101)  # for reproducibility shap <- explain(fit, X = subset(mtcars, select = -mpg), nsim = 10,                  pred_wrapper = predict) shap #>                             cyl       disp          hp         drat          wt #> Mazda RX4            0.08915238 -0.6278231  1.84101760  0.248727067  1.82607188 #> Mazda RX4 Wag        0.04457619 -0.8793257  0.80772767  0.220391072  1.08858405 #> Datsun 710           0.20059286 -2.2013814  0.63587072  0.085007985  3.68372384 #> Hornet 4 Drive       0.06686429  0.3607182  0.62083324 -0.499028356 -0.92956904 #> Hornet Sportabout   -0.15601667  1.9585467  0.19763549 -0.365219491  0.04458365 #> Valiant              0.06686429 -0.1373530  0.99891853 -0.898093619 -1.44896853 #> Duster 360          -0.24516905  1.8997383 -1.77871945 -0.318779944 -1.07186518 #> Merc 240D            0.15601667 -0.9765396  0.92158290  0.173951525  0.98455554 #> Merc 230             0.24516905 -0.3097776  2.14821190  0.168441748 -0.10588616 #> Merc 280            -0.13372857 -1.1133592  0.59290648  0.212519963 -0.18019224 #> Merc 280C           -0.02228810 -0.9728058  0.16756053  0.181035524  0.59073332 #> Merc 450SE          -0.11144048  0.4552651 -0.99677032 -0.437633701 -4.44796186 #> Merc 450SL          -0.22288096  0.8639902  0.14178199 -0.354199938 -2.25147418 #> Merc 450SLC         -0.20059286  0.9668049 -0.90010079 -0.557274568 -3.25089094 #> Cadillac Fleetwood  -0.22288096  4.2135358 -1.90116753 -0.513983465 -7.83743364 #> Lincoln Continental -0.22288096  3.0380344 -0.93662039 -0.327438164 -7.12595293 #> Chrysler Imperial   -0.15601667  2.9020149 -1.34478065 -0.391194153 -6.58389009 #> Fiat 128             0.26745715 -2.4382153  2.00857813  0.296740837  4.34876325 #> Honda Civic          0.26745715 -1.2279089  1.68419813  1.032689596  6.42376049 #> Toyota Corolla       0.26745715 -2.9140166  1.37915204  0.469118139  4.62926869 #> Toyota Corona        0.28974524 -1.4910132  0.91943469  0.241643068  3.44928817 #> Dodge Challenger    -0.24516905  0.9469354 -0.37593708 -0.510835021 -1.50284044 #> AMC Javelin         -0.15601667 -0.2817736 -0.17615338 -0.054310657  2.61706009 #> Camaro Z28          -0.28974524  1.6453019 -2.16539759  0.205435964 -1.29849872 #> Pontiac Firebird    -0.15601667  1.8290615 -0.53920119 -0.499815467 -1.63287608 #> Fiat X1-9            0.22288096 -1.8514647  2.04939415  0.310908834  3.81673173 #> Porsche 914-2        0.33432143 -1.4472736  0.90654542  0.743032758  2.68802239 #> Lotus Europa         0.26745715 -1.3448589  1.43930197 -0.161357749  6.18783869 #> Ford Pantera L      -0.08915238  1.0112112 -2.52200077  0.560423012  0.76906791 #> Ferrari Dino         0.13372857 -1.5195506  0.01933391  0.003935555  2.20986278 #> Maserati Bora       -0.15601667  0.4742011 -4.44250221  0.124363534 -1.15954636 #> Volvo 142E           0.24516905 -1.5504883  1.46078409  0.002361333  0.70590775 #>                             qsec          vs         am       gear        carb #> Mazda RX4           -0.538602732 -0.09532884  1.0080908  0.1310826 -0.05982578 #> Mazda RX4 Wag       -0.247133266 -0.09532884  1.5121361  0.1310826 -0.03988385 #> Datsun 710           0.635485540  0.22243397  1.5121361  0.2621652  0.39883851 #> Hornet 4 Drive       0.192944576  0.12710513 -1.2601134 -0.5898717  0.43872236 #> Hornet Sportabout   -1.018911570 -0.12710513 -0.2520227 -0.7209543  0.25924503 #> Valiant              2.009907755  0.22243397 -1.0080908 -0.3277065  0.19941925 #> Duster 360          -1.292318140 -0.22243397 -1.7641588 -0.4587891 -0.35895466 #> Merc 240D            2.382660256  0.15888141 -1.2601134  0.1310826 -0.03988385 #> Merc 230             3.256247613  0.15888141 -0.5040454  0.1310826  0.07976770 #> Merc 280             0.708558167  0.25421025 -1.0080908  0.2621652 -0.29912888 #> Merc 280C            1.219245513  0.19065769 -0.7560681  0.2621652 -0.37889658 #> Merc 450SE           0.005747285 -0.06355256 -0.5040454 -0.6554130 -0.01994193 #> Merc 450SL          -0.242207021 -0.15888141 -1.0080908 -0.0655413  0.01994193 #> Merc 450SLC          0.408057253 -0.03177628 -1.5121361 -0.2621652 -0.01994193 #> Cadillac Fleetwood  -0.118229868 -0.15888141 -1.5121361 -0.4587891 -0.31907081 #> Lincoln Continental -0.082925116 -0.12710513 -1.7641588 -0.7209543 -0.43872236 #> Chrysler Imperial   -1.283286692 -0.22243397 -0.7560681 -0.1966239 -0.27918696 #> Fiat 128             0.976217451  0.22243397  2.0161815  0.1966239  0.35895466 #> Honda Civic         -0.569802280  0.15888141  1.7641588  0.1966239  0.11965155 #> Toyota Corolla       1.612524032  0.15888141  1.5121361  0.0655413  0.51849006 #> Toyota Corona        1.752922001  0.19065769 -1.7641588 -0.5243304  0.45866429 #> Dodge Challenger    -0.713484411 -0.09532884 -0.5040454 -0.3277065  0.21936118 #> AMC Javelin         -0.961438718 -0.12710513 -0.7560681 -0.3277065  0.23930311 #> Camaro Z28          -2.207778576 -0.15888141 -1.0080908 -0.2621652 -0.25924503 #> Pontiac Firebird    -1.367853889 -0.09532884 -0.7560681 -0.3932478  0.27918696 #> Fiat X1-9            0.860450706  0.19065769  2.0161815  0.3932478  0.27918696 #> Porsche 914-2       -1.616629236 -0.06355256  1.7641588  0.9831195  0.19941925 #> Lotus Europa        -0.734831471  0.22243397  1.7641588  0.8520369  0.17947733 #> Ford Pantera L      -3.224227024 -0.09532884  1.2601134  0.8520369 -0.21936118 #> Ferrari Dino        -2.426996456 -0.22243397  1.7641588  1.0486608 -0.45866429 #> Maserati Bora       -3.036208692 -0.19065769  1.5121361  0.9175782 -1.03698013 #> Volvo 142E           0.149429416  0.06355256  1.0080908  0.0655413  0.09970963 #> attr(,\"baseline\") #> [1] 0 #> attr(,\"class\") #> [1] \"explain\" \"matrix\"  \"array\"    # Compute exact Shapley (i.e., LinearSHAP) values shap <- explain(fit, exact = TRUE) shap #>                             cyl        disp          hp        drat #> Mazda RX4            0.02089509 -0.94309317  0.78812524  0.23883899 #> Mazda RX4 Wag        0.02089509 -0.94309317  0.78812524  0.23883899 #> Datsun 710           0.24377605 -1.63652565  1.15332126  0.19948344 #> Hornet 4 Drive       0.02089509  0.36376034  0.78812524 -0.40659201 #> Hornet Sportabout   -0.20198587  1.72395481 -0.60821249 -0.35149424 #> Valiant              0.02089509 -0.07630258  0.89553584 -0.65846752 #> Duster 360          -0.20198587  1.72395481 -2.11196082 -0.30426759 #> Merc 240D            0.24377605 -1.12045186  1.81926695  0.07354568 #> Merc 230             0.24377605 -1.19912978  1.11035703  0.25458121 #> Merc 280             0.02089509 -0.84174535  0.50885769  0.25458121 #> Merc 280C            0.02089509 -0.84174535  0.50885769  0.25458121 #> Merc 450SE          -0.20198587  0.60112761 -0.71562309 -0.41446312 #> Merc 450SL          -0.20198587  0.60112761 -0.71562309 -0.41446312 #> Merc 450SLC         -0.20198587  0.60112761 -0.71562309 -0.41446312 #> Cadillac Fleetwood  -0.20198587  3.21750168 -1.25267606 -0.52465866 #> Lincoln Continental -0.20198587  3.05747880 -1.46749725 -0.46956089 #> Chrysler Imperial   -0.20198587  2.79077401 -1.78972904 -0.28852537 #> Fiat 128             0.24377605 -2.02724818  1.73333848  0.38051896 #> Honda Civic          0.24377605 -2.06725389  2.03408814  1.04956329 #> Toyota Corolla       0.24377605 -2.12859600  1.75482059  0.49071450 #> Toyota Corona        0.24377605 -1.47516924  1.06739279  0.08141679 #> Dodge Challenger    -0.20198587  1.16387474 -0.07115952 -0.65846752 #> AMC Javelin         -0.20198587  0.97718138 -0.07115952 -0.35149424 #> Camaro Z28          -0.20198587  1.59060241 -2.11196082  0.10503012 #> Pontiac Firebird    -0.20198587  2.25736441 -0.60821249 -0.40659201 #> Fiat X1-9            0.24377605 -2.02324760  1.73333848  0.38051896 #> Porsche 914-2        0.24377605 -1.47250219  1.19628550  0.65600780 #> Lotus Europa         0.24377605 -1.80855024  0.72367888  0.13651456 #> Ford Pantera L      -0.20198587  1.60393765 -2.52012108  0.49071450 #> Ferrari Dino         0.02089509 -1.14312177 -0.60821249  0.01844791 #> Maserati Bora       -0.20198587  0.93717566 -4.04535153 -0.04452096 #> Volvo 142E           0.24377605 -1.46316753  0.80960736  0.40413229 #>                               wt        qsec         vs        am       gear #> Mazda RX4            2.218965271 -1.14022034 -0.1390212  1.496385  0.2048166 #> Mazda RX4 Wag        1.271562769 -0.68043752 -0.1390212  1.496385  0.2048166 #> Datsun 710           3.333556450  0.62501727  0.1787416  1.496385  0.2048166 #> Hornet 4 Drive       0.008359434  1.30648109  0.1787416 -1.023842 -0.4505964 #> Hornet Sportabout   -0.827583950 -0.68043752 -0.1390212 -1.023842 -0.4505964 #> Valiant             -0.901890029  1.94689288  0.1787416 -1.023842 -0.4505964 #> Duster 360          -1.310573461 -1.64926561 -0.1390212 -1.023842 -0.4505964 #> Merc 240D            0.101242032  1.76626391  0.1787416 -1.023842  0.2048166 #> Merc 230             0.249854189  4.14728209  0.1787416 -1.023842  0.2048166 #> Merc 280            -0.827583950  0.37049464  0.1787416 -1.023842  0.2048166 #> Merc 280C           -0.827583950  0.86311909  0.1787416 -1.023842  0.2048166 #> Merc 450SE          -3.168225425 -0.36844204 -0.1390212 -1.023842 -0.4505964 #> Merc 450SL          -1.905022089 -0.20423389 -0.1390212 -1.023842 -0.4505964 #> Merc 450SLC         -2.090787286  0.12418241 -0.1390212 -1.023842 -0.4505964 #> Cadillac Fleetwood  -7.552284060  0.10776160 -0.1390212 -1.023842 -0.4505964 #> Lincoln Continental -8.198746944 -0.02360492 -0.1390212 -1.023842 -0.4505964 #> Chrysler Imperial   -7.905237933 -0.35202122 -0.1390212 -1.023842 -0.4505964 #> Fiat 128             3.779392921  1.33111232  0.1787416  1.496385  0.2048166 #> Honda Civic          5.952845719  0.55112360  0.1787416  1.496385  0.2048166 #> Toyota Corolla       5.135478855  1.68415984  0.1787416  1.496385  0.2048166 #> Toyota Corona        2.794837380  1.77447432  0.1787416 -1.023842 -0.4505964 #> Dodge Challenger    -1.124808264 -0.80359363 -0.1390212 -1.023842 -0.4505964 #> AMC Javelin         -0.809007430 -0.45054611 -0.1390212 -1.023842 -0.4505964 #> Camaro Z28          -2.313705521 -2.00231313 -0.1390212 -1.023842 -0.4505964 #> Pontiac Firebird    -2.332282041 -0.65580630 -0.1390212 -1.023842 -0.4505964 #> Fiat X1-9            4.763948462  0.86311909  0.1787416  1.496385  0.2048166 #> Porsche 914-2        4.002311157 -0.94317056 -0.1390212  1.496385  0.8602296 #> Lotus Europa         6.331806720 -0.77896241  0.1787416  1.496385  0.8602296 #> Ford Pantera L       0.175548111 -2.74946021 -0.1390212  1.496385  0.8602296 #> Ferrari Dino         1.661669682 -1.92841946 -0.1390212  1.496385  0.8602296 #> Maserati Bora       -1.310573461 -2.66735614 -0.1390212  1.496385  0.8602296 #> Volvo 142E           1.624516643  0.61680686  0.1787416  1.496385  0.2048166 #>                            carb #> Mazda RX4           -0.23681037 #> Mazda RX4 Wag       -0.23681037 #> Datsun 710           0.36144740 #> Hornet 4 Drive       0.36144740 #> Hornet Sportabout    0.16202814 #> Valiant              0.36144740 #> Duster 360          -0.23681037 #> Merc 240D            0.16202814 #> Merc 230             0.16202814 #> Merc 280            -0.23681037 #> Merc 280C           -0.23681037 #> Merc 450SE          -0.03739111 #> Merc 450SL          -0.03739111 #> Merc 450SLC         -0.03739111 #> Cadillac Fleetwood  -0.23681037 #> Lincoln Continental -0.23681037 #> Chrysler Imperial   -0.23681037 #> Fiat 128             0.36144740 #> Honda Civic          0.16202814 #> Toyota Corolla       0.36144740 #> Toyota Corona        0.36144740 #> Dodge Challenger     0.16202814 #> AMC Javelin          0.16202814 #> Camaro Z28          -0.23681037 #> Pontiac Firebird     0.16202814 #> Fiat X1-9            0.36144740 #> Porsche 914-2        0.16202814 #> Lotus Europa         0.16202814 #> Ford Pantera L      -0.23681037 #> Ferrari Dino        -0.63564887 #> Maserati Bora       -1.03448738 #> Volvo 142E           0.16202814 #> attr(,\"constant\") #> [1] 20.09062 #> attr(,\"baseline\") #> [1] 20.09062 #> attr(,\"class\") #> [1] \"explain\" \"matrix\"  \"array\""},{"path":"/reference/fastshap-package.html","id":null,"dir":"Reference","previous_headings":"","what":"fastshap: Fast Approximate Shapley Values — fastshap-package","title":"fastshap: Fast Approximate Shapley Values — fastshap-package","text":"Computes fast (relative implementations) approximate Shapley values supervised learning model. Shapley values help explain predictions black box model using ideas game theory; see Strumbel Kononenko (2014) doi:10.1007/s10115-013-0679-x  details.","code":""},{"path":[]},{"path":"/reference/fastshap-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"fastshap: Fast Approximate Shapley Values — fastshap-package","text":"Maintainer: Brandon Greenwell greenwell.brandon@gmail.com (ORCID)","code":""},{"path":"/reference/gen_friedman.html","id":null,"dir":"Reference","previous_headings":"","what":"Friedman benchmark data — gen_friedman","title":"Friedman benchmark data — gen_friedman","text":"Simulate data Friedman 1 benchmark problem. data originally described Friedman (1991) Breiman (1996). details, see sklearn.datasets.make_friedman1.","code":""},{"path":"/reference/gen_friedman.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Friedman benchmark data — gen_friedman","text":"","code":"gen_friedman(   n_samples = 100,   n_features = 10,   n_bins = NULL,   sigma = 0.1,   seed = NULL )"},{"path":"/reference/gen_friedman.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Friedman benchmark data — gen_friedman","text":"n_samples Integer specifying number samples (.e., rows) generate. Default 100. n_features Integer specifying number features generate. Default 10. n_bins Integer specifying number (roughly) equal sized bins split response . Default NULL binning. Setting positive integer > 1 effectively turns classification problem n_bins gives number classes. sigma Numeric specifying standard deviation noise. seed Integer specifying random seed. NULL (default) results different time function run.","code":""},{"path":"/reference/gen_friedman.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Friedman benchmark data — gen_friedman","text":"function mostly used internal testing.","code":""},{"path":"/reference/gen_friedman.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Friedman benchmark data — gen_friedman","text":"Breiman, Leo (1996) Bagging predictors. Machine Learning 24, pages 123-140. Friedman, Jerome H. (1991) Multivariate adaptive regression splines. Annals Statistics 19 (1), pages 1-67.","code":""},{"path":"/reference/gen_friedman.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Friedman benchmark data — gen_friedman","text":"","code":"gen_friedman() #>             y          x1          x2         x3          x4         x5 #> 1   12.508680 0.858668992 0.559547173 0.44641937 0.114472353 0.26883421 #> 2   22.479448 0.365129990 0.977672412 0.31493669 0.919276896 0.72702007 #> 3   10.925940 0.103726123 0.524608764 0.85950756 0.193799939 0.94570438 #> 4    5.782480 0.085171812 0.468667488 0.43013119 0.196065944 0.49261993 #> 5   13.572376 0.331509704 0.187139478 0.04316896 0.503607209 0.47479839 #> 6   13.622897 0.404571955 0.567182346 0.37642299 0.496971598 0.34158931 #> 7   22.652722 0.755830705 0.397415219 0.95996771 0.868919721 0.34366171 #> 8   11.823684 0.448214682 0.034019897 0.20195121 0.837611572 0.25373095 #> 9   12.364221 0.898853394 0.545338688 0.56594351 0.071882137 0.32792993 #> 10  18.509250 0.910393552 0.570173427 0.58114354 0.579575365 0.56101824 #> 11  19.581185 0.931327082 0.157349970 0.15233267 0.941101965 0.67870302 #> 12  20.996277 0.344829321 0.450736267 0.13165567 0.862870003 0.99259509 #> 13   7.010502 0.050779365 0.637689278 0.74416456 0.098535705 0.80865547 #> 14  13.928416 0.430085238 0.646409326 0.43200033 0.286782792 0.66004064 #> 15  12.078871 0.957321461 0.117038271 0.21170225 0.688989923 0.02158931 #> 16  14.022408 0.365097411 0.254932032 0.02619193 0.533626783 0.27863491 #> 17  12.768443 0.001666045 0.788766653 0.94987475 0.548818200 0.64188536 #> 18   9.704721 0.320671841 0.053895883 0.20696044 0.431188655 0.60441020 #> 19  18.051693 0.682753002 0.679645638 0.80375524 0.152114708 0.96365634 #> 20  15.039985 0.930875384 0.735856461 0.91716723 0.315546534 0.03721440 #> 21  20.523721 0.657430458 0.771374385 0.20824197 0.477443808 0.81775137 #> 22   7.176765 0.500102438 0.067876093 0.29412806 0.480594089 0.10963732 #> 23   6.456943 0.009032334 0.554375954 0.11393196 0.032965228 0.58478511 #> 24  10.791089 0.642982834 0.146903975 0.12090960 0.159241601 0.64205520 #> 25  22.449037 0.319247328 0.571024061 0.95704243 0.860235933 0.86186874 #> 26  14.180009 0.480398061 0.996407035 0.23076795 0.190828813 0.18418794 #> 27  14.555904 0.397417146 0.404322280 0.51928329 0.574724406 0.82476032 #> 28  22.320379 0.517558023 0.619875930 0.13135713 0.767586907 0.70577106 #> 29  16.150871 0.491005711 0.269674344 0.71591575 0.999464656 0.26963866 #> 30  19.595665 0.744847329 0.573693378 0.81967248 0.451328604 0.66334374 #> 31  15.499408 0.502751975 0.964870630 0.71062452 0.161639933 0.60051660 #> 32  13.217463 0.082020628 0.829110332 0.62156900 0.809243394 0.56232010 #> 33  10.977762 0.384788962 0.425176601 0.45384693 0.211751275 0.81148620 #> 34  12.995386 0.299486726 0.187966311 0.63738146 0.843769899 0.52058233 #> 35  20.545178 0.707541597 0.687992336 0.05860084 0.324966174 0.68195301 #> 36  18.388314 0.382288978 0.812758865 0.81914328 0.419223524 0.75348009 #> 37  22.440360 0.778512126 0.498806096 0.29352908 0.764182168 0.88751963 #> 38   7.794418 0.545759286 0.101318620 0.07878174 0.081933820 0.32056389 #> 39  21.978532 0.522748704 0.955017384 0.23655131 0.580539844 0.94629945 #> 40   7.873652 0.093352068 0.281990444 0.50936817 0.259618728 0.87237707 #> 41  16.759925 0.496863673 0.692443485 0.66762668 0.526440626 0.44234496 #> 42  15.847998 0.956699395 0.209484261 0.23376951 0.584071226 0.55777959 #> 43  12.026150 0.182177860 0.507534054 0.19657654 0.367237477 0.68927400 #> 44  15.129697 0.454814634 0.399530661 0.90207904 0.557116035 0.18349048 #> 45   5.372712 0.001679344 0.037090355 0.77952886 0.018385845 0.73252447 #> 46   9.076911 0.283297111 0.026584951 0.76425219 0.461246487 0.57020258 #> 47  21.484522 0.489691680 0.849759179 0.11541874 0.433595991 0.90693172 #> 48  11.312284 0.696521314 0.282902607 0.42882830 0.159674526 0.78049270 #> 49  17.273258 0.539397985 0.325768267 0.86478927 0.759668957 0.33276620 #> 50  13.027660 0.225982199 0.724213712 0.71015591 0.609967528 0.23674135 #> 51  11.351598 0.841800509 0.851606604 0.71209621 0.003243543 0.56744912 #> 52   9.174246 0.031045103 0.089644126 0.25331318 0.584412522 0.36480918 #> 53  16.887046 0.373930389 0.948610179 0.20503417 0.128293821 0.99499923 #> 54   9.857595 0.056807871 0.756669916 0.11691360 0.464448310 0.18755098 #> 55  15.448272 0.575660014 0.493528102 0.14991364 0.221891686 0.58988297 #> 56  15.547899 0.241541837 0.749761216 0.87254249 0.404951417 0.67663918 #> 57  18.201301 0.770400164 0.741892368 0.54591269 0.787077444 0.08854305 #> 58   9.955444 0.355248712 0.320497472 0.30846898 0.230879235 0.67643045 #> 59  14.923140 0.568178736 0.413964914 0.30057844 0.520828138 0.44168643 #> 60  10.828814 0.157875733 0.149543938 0.58239023 0.819659144 0.37673887 #> 61  19.229000 0.493801547 0.851782544 0.69674931 0.551280160 0.64222239 #> 62  12.626886 0.249440704 0.374908131 0.16730034 0.630344710 0.27382997 #> 63   6.413295 0.247185199 0.629831575 0.64994109 0.103227024 0.04987249 #> 64  13.269684 0.296022402 0.832694096 0.34762904 0.414592361 0.32098295 #> 65  15.817564 0.391862531 0.483622195 0.17233375 0.490723182 0.59577008 #> 66  14.653174 0.077176317 0.907962732 0.34487209 0.725947730 0.98023589 #> 67  12.982949 0.032286308 0.806289435 0.09093113 0.467668503 0.78371463 #> 68  15.019160 0.124697410 0.449162645 0.35590847 0.973223491 0.65047955 #> 69  15.743908 0.902870931 0.374740696 0.62736707 0.437304459 0.46466238 #> 70  23.624663 0.571252657 0.863349679 0.05723549 0.563774737 0.80691024 #> 71  19.782515 0.679020305 0.555323428 0.81265638 0.719851143 0.26165719 #> 72  10.169155 0.284378762 0.430759853 0.45372034 0.399309313 0.45706399 #> 73  21.417601 0.212584526 0.592008602 0.97325969 0.909621902 0.82811839 #> 74  10.571819 0.192024343 0.729179574 0.54533896 0.353317087 0.53638365 #> 75  22.526564 0.861673013 0.696993835 0.53080424 0.832413354 0.92924008 #> 76   2.465921 0.175815796 0.275465441 0.43311288 0.011521178 0.15389867 #> 77  14.327491 0.229394846 0.853227049 0.63783762 0.781052889 0.05476633 #> 78  10.136255 0.012182954 0.812571332 0.63352460 0.479548279 0.90632909 #> 79   9.949873 0.712006683 0.006141337 0.27782663 0.576211812 0.62040683 #> 80  16.461274 0.158334760 0.079724933 0.91076996 0.811576466 0.88508876 #> 81   9.717952 0.798977884 0.213373322 0.31338754 0.373981104 0.07923746 #> 82  11.468756 0.038454709 0.709827786 0.39716112 0.652164470 0.75734289 #> 83  15.646886 0.941182600 0.299699161 0.05470572 0.259163060 0.28550083 #> 84  21.449431 0.930206709 0.451143171 0.34604120 0.948179735 0.37879719 #> 85  14.446843 0.297610674 0.238596041 0.58709939 0.981950989 0.46466859 #> 86  12.606177 0.405535535 0.153773182 0.98421118 0.179118798 0.81590255 #> 87   6.826026 0.449557665 0.116008421 0.16548664 0.092207920 0.41431371 #> 88   8.508168 0.064139278 0.391936723 0.65188096 0.645617366 0.16729453 #> 89  16.410573 0.705599427 0.866964339 0.38207451 0.208205299 0.90355840 #> 90   9.299456 0.339410211 0.993090863 0.53831244 0.037761439 0.03827034 #> 91  19.538828 0.395263129 0.551627719 0.13800700 0.874792115 0.37004399 #> 92  13.922263 0.802358415 0.533529358 0.24660275 0.230674258 0.11654341 #> 93  23.741087 0.376943605 0.698485296 0.84286106 0.946045800 0.95764046 #> 94  18.145098 0.915326283 0.299003245 0.39455179 0.743961505 0.57569809 #> 95   4.309378 0.076202628 0.350360219 0.41606671 0.308272643 0.02480323 #> 96  14.798185 0.634925068 0.392326640 0.46337813 0.475403720 0.57806394 #> 97  14.316231 0.396530972 0.911453800 0.47688166 0.457358819 0.12437911 #> 98  25.771467 0.998634673 0.252213487 0.03661304 0.987825487 0.86902576 #> 99  23.826581 0.761909639 0.688889077 0.08890256 0.716177621 0.63659226 #> 100 17.268911 0.630840339 0.938761205 0.30205448 0.372012045 0.68325526 #>             x6         x7         x8         x9        x10 #> 1   0.07165747 0.39551818 0.95975666 0.25663405 0.24485833 #> 2   0.00506768 0.98025986 0.38012006 0.81135264 0.60811992 #> 3   0.73313299 0.98556535 0.29921583 0.43748583 0.96198649 #> 4   0.22492684 0.82539051 0.25514989 0.37421362 0.25738971 #> 5   0.18960039 0.68630278 0.65532167 0.16382068 0.50554709 #> 6   0.40606224 0.44025807 0.42315458 0.01120894 0.04240819 #> 7   0.78580072 0.29486931 0.79519206 0.54319015 0.49264079 #> 8   0.34755368 0.80917519 0.05257739 0.92182283 0.81824702 #> 9   0.31815393 0.57611395 0.97066507 0.66791307 0.45828255 #> 10  0.57476823 0.66977827 0.88047194 0.90211481 0.35325858 #> 11  0.30807186 0.38874647 0.18924702 0.98041681 0.23652354 #> 12  0.77976778 0.69652217 0.30929188 0.13530614 0.66959548 #> 13  0.28295991 0.83467609 0.18744677 0.37985216 0.83954121 #> 14  0.28858567 0.18059869 0.95820326 0.30903893 0.13615496 #> 15  0.40712082 0.18472799 0.35230657 0.10692049 0.90858457 #> 16  0.79316410 0.32213954 0.43409567 0.24259980 0.89235823 #> 17  0.41750890 0.13291428 0.99649488 0.32603652 0.68136199 #> 18  0.84669258 0.67660278 0.19518177 0.27183690 0.73360897 #> 19  0.47378925 0.43753377 0.66973329 0.66194722 0.25521597 #> 20  0.94889092 0.92196911 0.07530727 0.24643390 0.07813625 #> 21  0.66919885 0.37611235 0.81548251 0.21385616 0.59287512 #> 22  0.84765306 0.79002407 0.61909664 0.68289349 0.24328921 #> 23  0.09697748 0.36047047 0.91921143 0.98937228 0.41543900 #> 24  0.60490684 0.78595350 0.79985754 0.36894349 0.47838790 #> 25  0.87615991 0.54277126 0.23636800 0.32375046 0.91971897 #> 26  0.90773901 0.44800020 0.21575081 0.46000705 0.71999931 #> 27  0.83663322 0.76108976 0.80057669 0.43853908 0.97012592 #> 28  0.15971878 0.78220086 0.66522152 0.10288856 0.08664368 #> 29  0.85286586 0.40875337 0.19086674 0.53272029 0.63726236 #> 30  0.46549883 0.86738173 0.23058056 0.14976423 0.54332376 #> 31  0.54343594 0.69176764 0.89991088 0.43913373 0.98715108 #> 32  0.63039015 0.15828128 0.58239408 0.88878360 0.72324079 #> 33  0.73280523 0.45646685 0.19728039 0.01408781 0.72263306 #> 34  0.73458861 0.36662890 0.76954915 0.82604523 0.28638125 #> 35  0.84726952 0.96279897 0.72394400 0.91646300 0.82434517 #> 36  0.73189228 0.20754493 0.19744470 0.49806293 0.18227235 #> 37  0.44682491 0.07538959 0.92760940 0.28126380 0.77857724 #> 38  0.79942195 0.81743462 0.52397709 0.25875392 0.61138061 #> 39  0.42703535 0.68146272 0.21239877 0.30456213 0.66772947 #> 40  0.14469106 0.85887412 0.93555341 0.85352276 0.33530881 #> 41  0.36429581 0.77352280 0.82291152 0.18313840 0.12500351 #> 42  0.10655865 0.19764348 0.66773231 0.21786342 0.84487347 #> 43  0.10769924 0.38756346 0.93887399 0.32069146 0.33839679 #> 44  0.69793217 0.88319434 0.53045958 0.60608329 0.73493665 #> 45  0.67109315 0.95695499 0.13813218 0.13577084 0.98406365 #> 46  0.25479387 0.02857349 0.15740556 0.21957998 0.70400391 #> 47  0.32965172 0.69772892 0.61373539 0.43857195 0.72588599 #> 48  0.33571618 0.33698020 0.95530372 0.26382044 0.54524503 #> 49  0.45082380 0.71694316 0.67082462 0.27960620 0.11634905 #> 50  0.33950042 0.67664724 0.86754180 0.42280470 0.45810613 #> 51  0.55862081 0.46947430 0.06800344 0.44297234 0.43319813 #> 52  0.48893637 0.58574610 0.34843799 0.23591573 0.94301806 #> 53  0.27313027 0.54180234 0.11520502 0.76790000 0.58923175 #> 54  0.89396925 0.99099038 0.16132747 0.43032146 0.94466836 #> 55  0.75300594 0.85417555 0.37684322 0.32466043 0.36858399 #> 56  0.35496567 0.42082709 0.48872788 0.39758757 0.65726675 #> 57  0.32279429 0.53250537 0.76083148 0.71067803 0.55429472 #> 58  0.99667268 0.51529433 0.04403185 0.30772233 0.76978260 #> 59  0.86889496 0.54629661 0.23559907 0.62182639 0.83372063 #> 60  0.34983144 0.94049579 0.60705215 0.31833481 0.54649001 #> 61  0.65027636 0.62416372 0.61485522 0.50731407 0.56768617 #> 62  0.77134985 0.55635054 0.47047771 0.04501199 0.74624627 #> 63  0.31504905 0.02895887 0.29017977 0.62187347 0.16131237 #> 64  0.85960622 0.35654250 0.59887128 0.09110199 0.92422739 #> 65  0.87483393 0.18930051 0.77635277 0.64430303 0.52655862 #> 66  0.29707242 0.45189174 0.65548439 0.97762440 0.53292717 #> 67  0.37242949 0.29719023 0.49479629 0.90938963 0.61519795 #> 68  0.50678112 0.93996683 0.44431773 0.37913860 0.77580450 #> 69  0.95742220 0.46972229 0.27944051 0.01553222 0.07706682 #> 70  0.84861959 0.45748225 0.20668650 0.86026934 0.48846376 #> 71  0.39614118 0.64567136 0.41121381 0.52480811 0.39158442 #> 72  0.25367087 0.15977975 0.11358950 0.29841301 0.99482200 #> 73  0.56057679 0.19041209 0.44689108 0.63792671 0.48162075 #> 74  0.53585050 0.62202679 0.22300473 0.85884735 0.48588316 #> 75  0.44178868 0.39310232 0.97342649 0.34107576 0.27968173 #> 76  0.19706366 0.02584121 0.51903712 0.46497696 0.61811320 #> 77  0.46223204 0.41564055 0.13673048 0.47923708 0.04855309 #> 78  0.23599433 0.80101465 0.30976491 0.56934895 0.58626519 #> 79  0.70572187 0.74256944 0.31762786 0.27915926 0.34722234 #> 80  0.69161426 0.85717971 0.94143387 0.44141478 0.99901984 #> 81  0.74722217 0.17841130 0.69882645 0.47337626 0.93131478 #> 82  0.15013919 0.68249250 0.78471219 0.33156671 0.55427198 #> 83  0.97291946 0.76831187 0.28953431 0.59260418 0.14586199 #> 84  0.72325986 0.49824897 0.44012611 0.61771962 0.62978833 #> 85  0.17051974 0.84494426 0.78194708 0.85940546 0.30844450 #> 86  0.92658357 0.41191996 0.45711269 0.05744917 0.56177192 #> 87  0.59394242 0.71329566 0.86301159 0.28823774 0.51896421 #> 88  0.35856969 0.41445042 0.86820954 0.23218449 0.08037205 #> 89  0.39301791 0.11852654 0.22361964 0.26285183 0.52075961 #> 90  0.94613848 0.21326341 0.01485025 0.17452605 0.12289985 #> 91  0.21406932 0.68158200 0.11298782 0.33435814 0.25181524 #> 92  0.94343302 0.68937101 0.58247811 0.50110813 0.77907235 #> 93  0.96743037 0.81129464 0.85762794 0.86502249 0.14616594 #> 94  0.90698271 0.20449472 0.95157956 0.17254832 0.79418969 #> 95  0.19260592 0.92891082 0.12066013 0.56511970 0.82597229 #> 96  0.10226850 0.24194720 0.42585720 0.02449699 0.58804851 #> 97  0.89951044 0.59705196 0.97261963 0.89811145 0.87325399 #> 98  0.65789404 0.54022088 0.52420427 0.26925744 0.70684232 #> 99  0.97300168 0.67646808 0.68224856 0.08773706 0.54304839 #> 100 0.78656857 0.52933295 0.51728385 0.75885089 0.81413188"},{"path":"/reference/titanic.html","id":null,"dir":"Reference","previous_headings":"","what":"Survival of Titanic passengers — titanic","title":"Survival of Titanic passengers — titanic","text":"data set containing survival outcome, passenger class, age, sex, number family members large number passengers aboard ill-fated Titanic.","code":""},{"path":"/reference/titanic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Survival of Titanic passengers — titanic","text":"","code":"titanic"},{"path":"/reference/titanic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Survival of Titanic passengers — titanic","text":"data frame 1309 observations following 6 variables: survived binary levels \"yes\" survived \"\" otherwise; pclass integer giving corresponding passenger (.e., ticket) class values 1--3; age age years corresponding passenger (263 missing values); sex factor giving sex passenger levels \"male\" \"female\"; sibsp integer giving number siblings/spouses aboard passenger (ranges 0--8); parch integer giving number parents/children aboard passenger (ranges 0--9).","code":""},{"path":"/reference/titanic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Survival of Titanic passengers — titanic","text":"https://hbiostat.org/data/.","code":""},{"path":"/reference/titanic.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Survival of Titanic passengers — titanic","text":"mentioned column description, age contains 263 NAs (missing values). complete version (versions) data set, see titanic_mice.","code":""},{"path":"/reference/titanic_mice.html","id":null,"dir":"Reference","previous_headings":"","what":"Survival of Titanic passengers — titanic_mice","title":"Survival of Titanic passengers — titanic_mice","text":"titanic data set contains 263 missing values (.e., NA's) age column. version data contains imputed values age column using multivariate imputation chained equations via mice package. Consequently, list containing 11 imputed versions observations containd titanic data frame; completed data sets dimension column structure titanic.","code":""},{"path":"/reference/titanic_mice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Survival of Titanic passengers — titanic_mice","text":"","code":"titanic_mice"},{"path":"/reference/titanic_mice.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Survival of Titanic passengers — titanic_mice","text":"object class mild (inherits list) length 21.","code":""},{"path":"/reference/titanic_mice.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Survival of Titanic passengers — titanic_mice","text":"Greenwell, Brandon M. (2022). Tree-Based Methods Statistical Learning R. CRC Press.","code":""},{"path":[]},{"path":"/news/index.html","id":"breaking-changes-0-1-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"fastshap 0.1.0","text":"explain() function now returns matrix, opposed tibble, makes sense since Shapley values values ALWAYS numeric; data frames (tibbles’s) really necessary data heterogeneous. essence, output explain() act like R matrix class structure c(\"explain\", \"matrix\", \"array\"); always convert results tibble using tibble::as_tibble(result). Two new data sets, titanic titanic_mice, added package; see corresponding help pages details. plotting functions deprecated favor (far superior) shapviz package @Mayer79 (grid.arrange() also longer imported gridExtra). Consequently, output explain() longer needs \"explain\" class (ordinary c(\"matrix\", \"array\") object returned). explain() function gained three new arguments: baseline, defaults NULL, containing baseline use adjusting Shapley values meet efficiency property. NULL adjust = TRUE, default average training prediction (.e., average prediction X.) shap_only, defaults TRUE, determines whether return matrix Shapley values (TRUE) containing baseline aanattribute list containing Shapley values, corresponding feature values, baseline (FALSE); setting FALSE convenience using shapviz package. parallel, defaults FALSE determining whether compute Shapley values parallel (across features) using suitable parallel backend supported foreach.","code":""},{"path":"/news/index.html","id":"miscellaneous-0-1-0","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"fastshap 0.1.0","text":"X newdata arguments explain() now work tibble (#20). Minor change explain.lgb.Booster() support breaking changes lightgbm v4.0.0. (Thanks @jameslamb @Mayer79.) dependency matrixStats removed favor using R’s internal apply() var() functions. dependency plyr, retired, removed favor using foreach directly.","code":""},{"path":"/news/index.html","id":"fastshap-007","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.7","title":"fastshap 0.0.7","text":"CRAN release: 2021-12-06","code":""},{"path":"/news/index.html","id":"miscellaneous-0-0-7","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"fastshap 0.0.7","text":"Move lightgbm tests slowtests/ directory (now).","code":""},{"path":"/news/index.html","id":"fastshap-006","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.6","title":"fastshap 0.0.6","text":"CRAN release: 2021-12-03","code":""},{"path":"/news/index.html","id":"enhancements-0-0-6","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"fastshap 0.0.6","text":"Thanks Michael Mayer (@mayer79), function explain() now supports lightgbm models (#15).","code":""},{"path":"/news/index.html","id":"bug-fixes-0-0-6","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"fastshap 0.0.6","text":"force_plot() function now compatible shap (>=0.36.0); thanks @hfshr @jbwoillard reporting (#12). Fixed minor name repair issue caused tibble.","code":""},{"path":"/news/index.html","id":"miscellaneous-0-0-6","dir":"Changelog","previous_headings":"","what":"Miscellaneous","title":"fastshap 0.0.6","text":"Switched Travis-CI GitHub Actions continuous integration.","code":""},{"path":"/news/index.html","id":"fastshap-005","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.5","title":"fastshap 0.0.5","text":"CRAN release: 2020-02-02","code":""},{"path":"/news/index.html","id":"bug-fixes-0-0-5","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"fastshap 0.0.5","text":"Fixed bug occurred logical columns older version R (<= 3.6.0) (#9).","code":""},{"path":"/news/index.html","id":"fastshap-004","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.4","title":"fastshap 0.0.4","text":"CRAN release: 2020-01-26","code":""},{"path":"/news/index.html","id":"enhancements-0-0-4","dir":"Changelog","previous_headings":"","what":"Enhancements","title":"fastshap 0.0.4","text":"Function explain() now MUCH faster explaining single observation, especially nsim relatively large (e.g., nsim >= 1000).","code":""},{"path":"/news/index.html","id":"bug-fixes-0-0-4","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"fastshap 0.0.4","text":"Fixed MAJOR bug occurred whenever explaining data sets non-numeric features.","code":""},{"path":"/news/index.html","id":"new-features-0-0-4","dir":"Changelog","previous_headings":"","what":"New features","title":"fastshap 0.0.4","text":"default method explain() gained new logical argument called adjust. adjust = TRUE (nsim > 1), algorithm adjust sum estimated Shapley values satisfy efficiency property; , equal difference model’s prediction sample average prediction training data. option experimental follow approach shap (#6). New (experimental) function constructing force plots (#7) help visualize prediction explanations. function also generic means additional methods can added. Function explain() became generic gained new logical argument, exact, computing exact Shapley contributions linear models (Linear SHAP, assumes independent features) boosted decision trees (Tree SHAP). Currently, \"lm\", \"glm\", \"xgb.Booster\" objects supported (#2)(#3).","code":""},{"path":"/news/index.html","id":"minor-changes-0-0-4","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"fastshap 0.0.4","text":"Minor improvements package documentation. Removed unnecessary legend contribution plots.","code":""},{"path":"/news/index.html","id":"fastshap-003","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.3","title":"fastshap 0.0.3","text":"CRAN release: 2019-12-03","code":""},{"path":"/news/index.html","id":"minor-changes-0-0-3","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"fastshap 0.0.3","text":"Tweak imports (particular, use @importFrom Rcpp sourceCpp tag). Fixed typo package description; Shapley misspelled Shapely (fixed Dirk Eddelbuettel (#1)).","code":""},{"path":"/news/index.html","id":"fastshap-002","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.2","title":"fastshap 0.0.2","text":"CRAN release: 2019-11-22","code":""},{"path":"/news/index.html","id":"new-features-0-0-2","dir":"Changelog","previous_headings":"","what":"New features","title":"fastshap 0.0.2","text":"can now specify type = \"contribution\" call autoplot.fastshap() plot explanation single instance (controlled row_num argument). autoplot.fastshap() gained useful new arguments: color_by specifying additional feature color dependence plots (.e., whenever type = \"dependence\"); smooth, smooth_color, smooth_linetype, smooth_size, smooth_alpha adding/controlling smoother dependence plots (.e., whenever type = \"dependence\"). ... can used pass additional parameters geom_col() (type = \"importance\") geom_point() (type = \"dependence\").","code":""},{"path":"/news/index.html","id":"breaking-changes-0-0-2","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"fastshap 0.0.2","text":"Function fastshap() renamed explain(). Functions explain() explain_column() (currently exported) now throw error whenever inputs X newdata inherit class.","code":""},{"path":"/news/index.html","id":"bug-fixes-0-0-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"fastshap 0.0.2","text":"Fixed bug C++ source gave weight extreme permutations. Fixed bug C++ source caused doubles incorrectly converted integers. Fixed bug autoplot.fastshap() type = \"importance\"; particular, function incorrectly used sum(|Shapley value|) instead mean(|Shapley value|).","code":""},{"path":"/news/index.html","id":"fastshap-001","dir":"Changelog","previous_headings":"","what":"fastshap 0.0.1","title":"fastshap 0.0.1","text":"Initial release.","code":""}]
