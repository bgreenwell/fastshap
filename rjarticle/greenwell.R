# Generated by `rjournal_pdf_article()` using `knitr::purl()`: do not edit by hand
# Please edit greenwell.Rmd to modify this file

## ----setup, include=FALSE-----------------------------------------------------
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  fig.width = 6, 
  fig.asp = 0.618,
  out.width = "100%"
)


## ----drinks, echo=FALSE-------------------------------------------------------
library(kableExtra)

# Generate data frame of table values
combos <- c(
  "Alex, Brad, Brandon",
  "Alex, Brandon, Brad",
  "Brad, Alex, Brandon",
  "Brad, Brandon, Alex",
  "Brandon, Alex, Brad",
  "Brandon, Brad, Alex",
  "Shapley contribution:"
)
tab <- data.frame(
  `Permutation/order of players` = combos,
  "Alex"    = scales::dollar(c(10, 10, 5, 10, 5, 17, 9.50)),
  "Brad"    = scales::dollar(c(15, 15, 20, 20, 15, 3, 14.67)),
  "Brandon" = scales::dollar(c(5, 5, 5, 0, 10, 10, 5.83)),
  check.names = FALSE
)

# Figure caption
cap <- paste(
  "Marginal contribution for each permutation of the players",
  "{Alex, Brad, Brandon} (i.e., the order in which they arrive). The Shapley ",
  "contribution is the average marginal contribution cross all permutations.",
  "(Notice how each row sums to the total bill of $30.)"
)


## ----drinks-html, echo=FALSE, eval=knitr::is_html_output()--------------------
#> knitr::kable(tab, format = "html", align = c("lrrr"), caption = cap) %>%
#>   row_spec(6, hline_after = TRUE)

## ----drinks-pdf, echo=FALSE, eval=knitr::is_latex_output()--------------------
# cap <- gsub("\\{", replacement = "\\\\{", x = cap)
# cap <- gsub("\\}", replacement = "\\\\}", x = cap)
cap <- gsub("\\$", replacement = "\\\\$", x = cap)
knitr::kable(tab, format = "latex", align = c("lrrr"), caption = cap, 
             booktabs = TRUE, linesep = "", position = "!htb") %>%
  row_spec(6, hline_after = TRUE)


## ----sample-shap--------------------------------------------------------------
sample.shap <- function(f, obj, R, x, feature, X) {
  phi <- numeric(R)  # to store Shapley values
  N <- nrow(X)   # sample size
  p <- ncol(X)   # number of features
  b1 <- b2 <- x  # initialize new instances
  for (m in seq_len(R)) {
    w <- X[sample(N, size = 1), ]  # sample random obs from X     # (step 2. b.)
    ord <- sample(names(w))  # random permutation of features     #
    swap <- ord[seq_len(which(ord == feature) - 1)]               #
    b1[swap] <- w[swap]                                           # (step 1. c.)
    b2[c(swap, feature)] <- w[c(swap, feature)]                   # (step 1. c.)
    phi[m] <- f(obj, newdata = b1) - f(obj, newdata = b2)         # (step d.)
  }                                                               #
  mean(phi)  # return approximate feature contribution            # (step 2.)
}


## ----pkgsearch, cache=TRUE----------------------------------------------------
pkgsearch::ps("Shapley")  # set `format = "long"` for more detailed results


## ----ex-titanic-load, cache=TRUE----------------------------------------------
library(fastshap)

# Use one of fastshap's imputed versions of the Titanic data
head(titanic <- titanic_mice[[1L]])


## ----ex-titanic-lightgbm, cache=TRUE------------------------------------------
library(lightgbm)

# Re-encode binary variables as 0/1
titanic$survived <- ifelse(titanic$survived == "yes", 1, 0)
titanic$sex <- ifelse(titanic$sex == "male", 1, 0)
    
# Matrix of only predictor values
X <- data.matrix(subset(titanic, select = -survived))

params <- list(
  num_leaves = 10L,
  learning_rate = 0.1,
  objective = "binary"
)

set.seed(1420)  # for reproducibility
bst <- lightgbm(X, label = titanic$survived, params = params, nrounds = 45,
                verbose = 0)


## ----ex-titanic-jack, cache=TRUE----------------------------------------------
jack.dawson <- data.matrix(data.frame(
  #survived = 0L,  # in case you haven't seen the movie
  pclass = 3L,     # third-class passenger
  age = 20.0,      # twenty years old
  sex = 1L,        # male
  sibsp = 0L,      # no siblings/spouses aboard
  parch = 0L       # no parents/children aboard
))  # lightgbm doesn't like data frames


## ----ex-titanic-pfun, cache=TRUE----------------------------------------------
pfun <- function(object, newdata) {  # prediction wrapper
  predict(object, data = data.matrix(newdata), rawscore = TRUE)
}

# Compute Jack's predicted likelihood of survival
(jack.logit <- pfun(bst, newdata = jack.dawson))  # logit scale
(jack.prob <- plogis(jack.logit))  # probability scale


## ----ex-titanic-explain-lgb, cache=TRUE---------------------------------------
ex.lightgbm <- predict(bst, data = jack.dawson, predcontrib = TRUE)
colnames(ex.lightgbm) <- c(colnames(X), "baseline")
ex.lightgbm
sum(ex.lightgbm)  # since baseline is included, this should some to prediction


## ----ex-titanic-benchmark, echo=FALSE-----------------------------------------
benchmark <- readRDS("data/benchmark.rds")
maxy <- max(benchmark$iBreakDown, benchmark$iml, benchmark$fastshap)
palette("Okabe-Ito")
plot(iBreakDown ~ nreps, data = benchmark, type = "b", las = 1,
     xlab = "Number of Monte Carlo repetitions", ylab = "Time (in seconds)", 
     ylim = c(0, maxy))
lines(iml ~ nreps, data = benchmark, type = "b", col = 2)
lines(fastshap ~ nreps, data = benchmark, type = "b", col = 3)
legend("topleft", legend = c("iBreakDown", "iml", "fastshap"),
       lty = c(1, 1, 1), col = 1:3, inset = 0.02)
palette("default")


## ----als, cache=TRUE----------------------------------------------------------
als <- read.table("https://web.stanford.edu/~hastie/CASI_files/DATA/ALS.txt", 
                  header = TRUE)

# Split into train/test sets
als.trn <- als[!als$testset, -1]  # train
als.val <- als[als$testset, -1]  # validation

# Print dimensions
dim(als.trn)
dim(als.val)


## ----als-ngb, cache=TRUE------------------------------------------------------
library(reticulate)

ngboost <- import("ngboost")  # requires installation of ngboost

# Construct an NGBoost regressor object
ngb <- ngboost$NGBRegressor(
  Dist = ngboost$distns$Normal,
  n_estimators = 2000L,
  learning_rate = 0.01,
  verbose_eval = 0,
  random_state = 1601L
)


## ----als-ngb-fit, cache=TRUE--------------------------------------------------
X.trn <- subset(als.trn, select = -dFRS)  # features only
X.val <- subset(als.val, select = -dFRS)  # features only

# Train the model
ngb$fit(
  X = X.trn,
  Y = als.trn$dFRS,
  X_val = X.val,
  Y_val = als.val$dFRS,
  early_stopping_rounds = 5L
)


## ----als-ngb-predict, cache=TRUE----------------------------------------------
ngb$predict(X.val[1, ])
(params <- ngb$pred_dist(X.val[1, ])$params)


## ----als-ngb-normal, cache=TRUE, fig.cap="ABC."-------------------------------
plot(function(x) pnorm(x, mean = params$loc, sd = params$scale), 
     from = min(als$dFRS), to = max(als$dFRS),
     xlab = "dFRS", ylab = "Cumulative probability")


## ----als-ngboost-shap, cache=TRUE---------------------------------------------
library(ggplot2)
library(shapviz)

# Use 'shap' package to compute SHAP values for entire training set
shap <- import("shap")
explainer <- shap$TreeExplainer(ngb, model_output = 0L)
ex.trn <- explainer$shap_values(X.trn)  # training explanations
colnames(ex.trn) <- colnames(X.trn)

# Construct variable importance and SHAP dependence plots
viz <- shapviz(ex.trn, X = X.trn)
p1 <- sv_importance(viz) + theme_bw()
p2 <- sv_dependence(viz, v = "Onset.Delta", alpha = 0.3) + theme_bw()
gridExtra::grid.arrange(p1, p2, nrow = 1)


## ----als-ngboost-max, cache=TRUE----------------------------------------------
pfun <- function(object, newdata) {
  dist <- object$pred_dist(newdata)
  pnorm(-1.1, mean = dist$params$loc, sd = dist$params$scale)
}

max(prob.val <- pfun(ngb, newdata = X.val))
xval.max <- X.val[which.max(prob.val), ]  # obs with highest predicted prob


## ----als-ngboost-fastshap, cache=TRUE-----------------------------------------
library(fastshap)

system.time({
  set.seed(1110)
  ex <- explain(ngb, X = X.trn, nsim = 100, pred_wrapper = pfun, 
                newdata = xval.max, adjust = TRUE)
})

# Visualize with a force plot
viz <- shapviz(ex, X = xval.max, baseline = attr(ex, which = "baseline"))
sv_waterfall(viz, max_display = 10)

